{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947f8751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd        \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#scores = cross_val_score(model, X, y, cv=5) # will give negative score in some folds, bacause the rows are ordered\n",
    "#scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6e40c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = pd.read_csv('numerical.csv')\n",
    "file2 = pd.read_csv('categorical.csv')\n",
    "file3 = pd.read_csv('target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb3eaa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b132fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)                       # concate the three files\n",
    "file1_file3 = pd.concat((file1, file2, file3),axis=1)\n",
    "#file1_file3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb26b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost_data = file1_file3[file1_file3['TARGET_B']==1]\n",
    "# X = donations_data.drop(columns=['TARGET_B','TARGET_D'])\n",
    "# y = donations_data['TARGET_D']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069118ea",
   "metadata": {},
   "source": [
    "# Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35210174",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = file1_file3[['TARGET_B', 'TARGET_D']]\n",
    "X_num = file1_file3.drop(columns = ['TARGET_B', 'TARGET_D'])\n",
    "X_cat = file2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c09816",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_num, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95147e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split tthem more into numericals and categoricals\n",
    "X_train_num = X_train.select_dtypes(include = np.number)\n",
    "X_test_num  = X_test.select_dtypes(include = np.number)\n",
    "X_train_cat = X_train.select_dtypes(include = object)\n",
    "X_test_cat  = X_test.select_dtypes(include = object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d979a2ca",
   "metadata": {},
   "source": [
    "# Scaling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ea3623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively you could use StandardScaler:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "transformer = StandardScaler().fit(X_train_num)\n",
    "X_train_num_scaled = transformer.transform(X_train_num)\n",
    "print(X_train_num_scaled.shape)\n",
    "X_train_num_scaled= pd.DataFrame(X_train_num_scaled,columns=X_train_num.columns)\n",
    "X_train_num_scaled.head()\n",
    "X_train_num_scaled.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71857cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all features are numeric, so no need to split into _num and _cat\n",
    "#transformer = StandardScaler().fit(X_train)\n",
    "#X_train_scaled = pd.DataFrame(transformer.transform(X_train),columns=X.columns)\n",
    "# because this is the only tranformation we apply to the numerics, \n",
    "# we can immediately transform the X_test as well\n",
    "# X_test_num_scaled = pd.DataFrame(transformer.transform(X_test_num),columns=X_test_num.columns)\n",
    "# X_test_num_scaled.head()\n",
    "\n",
    "\n",
    "# applying scaler to X_test_num\n",
    "X_test_scaled_arr = transformer.transform(X_test_num)\n",
    "X_test_num_scaled = pd.DataFrame(X_test_scaled_arr, columns=X_test_num.columns)\n",
    "X_test_num_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc5295",
   "metadata": {},
   "source": [
    "# Encoding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cfa80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding categorical data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(drop='first').fit(X_train_cat)\n",
    "X_train_encoded_cat = encoder.transform(X_train_cat).toarray()\n",
    "cols = encoder.get_feature_names_out(input_features=X_train_cat.columns)\n",
    "\n",
    "X_train_onehot_encoded = pd.DataFrame(X_train_encoded_cat, columns=cols)\n",
    "X_train_onehot_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba2fea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying encoder to X_test_cat\n",
    "X_test_encoded_cat = encoder.transform(X_test_cat).toarray()\n",
    "X_test_onehot_encoded_cat = pd.DataFrame(X_test_encoded_cat, columns=cols)\n",
    "X_test_onehot_encoded_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5e1185",
   "metadata": {},
   "source": [
    "# Concatenating Scaled and Encoded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb0695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining transformed and encoded data togetther for prediction\n",
    "X_train_treated = pd.concat([X_train_num_scaled, X_train_onehot_encoded], axis=1)\n",
    "X_train_treated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3541b9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining transformed and encoded data togetther for prediction\n",
    "X_test_treated = pd.concat([X_test_num_scaled, X_test_onehot_encoded_cat], axis=1)\n",
    "X_test_treated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2343a17",
   "metadata": {},
   "source": [
    "# Running the Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d14f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# classification = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "#                   multi_class='multinomial').fit(X_train_transformed, y_train)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LogReg = LogisticRegression(random_state=0, solver='lbfgs')\n",
    "LogReg .fit(X_train_treated, y_train['TARGET_B'])\n",
    "LogReg .score(X_test_treated, y_test['TARGET_B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1875c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#while accuracy is not absolutely terrible, a closer look reveals some serious problems\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "pred = LogReg.predict(X_test_treated)\n",
    "\n",
    "print(\"accuracy:\"   ,accuracy_score(y_test['TARGET_B'], pred))\n",
    "print(\"precision: \",precision_score(y_test['TARGET_B'],pred, pos_label= 0))\n",
    "print(\"recall: \",recall_score(y_test['TARGET_B'],pred, pos_label= 0))\n",
    "print(\"f1: \",f1_score(y_test['TARGET_B'],pred, pos_label= 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0fd186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we a certain percentage of churn is unidentifiable\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test['TARGET_B'],pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa95f997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83ca76d0",
   "metadata": {},
   "source": [
    "# Handling Imbalance in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77ceec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while there are more imbalanced datasets, we have a significant imbalance\n",
    "# and the cost of failing to detect the minority class is quite high (lack of diagnosis of diabetes)\n",
    "count_classes = y_train['TARGET_B'].value_counts()\n",
    "count_classes\n",
    "count_classes.plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d31459",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)\n",
    "y_train.reset_index(drop=True, inplace = True)\n",
    "trainset = pd.concat((X_train_treated, y_train),axis=1)\n",
    "trainset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c4a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train['TARGET_B'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7590fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_B = trainset.drop(['TARGET_D'], axis=1).copy()\n",
    "# data_B.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6d537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "category_0 = trainset[ trainset['TARGET_B'] == 0]\n",
    "category_1 =  trainset[ trainset['TARGET_B'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ff4b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_1_oversampled = resample(category_1,\n",
    "                                  replace=True,\n",
    "                                  n_samples = len(category_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c893de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(category_0.shape)\n",
    "print(category_1_oversampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e32864",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_upsampled = pd.concat([category_0, category_1_oversampled], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e267fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_upsampled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86a0a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = data_upsampled.drop(['TARGET_B', 'TARGET_D'], axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0cec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_1 = data_upsampled['TARGET_B'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aae2f33",
   "metadata": {},
   "source": [
    "# Run the Regressor Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1934c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# LogReg = LogisticRegression(random_state=0, solver='lbfgs')\n",
    "# LogReg .fit(X_train_1, y_train['TARGET_B'])\n",
    "# LogReg .score(X_test_treated, y_test['TARGET_B'])\n",
    "\n",
    "\n",
    "# Our Logistic Regression, while still not amazing, has improved substantially!\n",
    "# especially at detecting instances of diabetes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LogReg = LogisticRegression(random_state=0, solver='lbfgs')\n",
    "LogReg .fit(X_train_1, y_train_1)\n",
    "LogReg .score(X_test_treated, y_test['TARGET_B'])\n",
    "\n",
    "LR_over = LogisticRegression(random_state=0, solver='lbfgs')\n",
    "LR_over.fit(X_train_1, y_train_1)\n",
    "pred = LR_over.predict(X_test_treated)\n",
    "\n",
    "pred = LogReg.predict(X_test_treated)\n",
    "\n",
    "print(\"accuracy:\"   ,accuracy_score(y_test['TARGET_B'], pred))\n",
    "print(\"precision: \",precision_score(y_test['TARGET_B'],pred, pos_label= 0))\n",
    "print(\"recall: \",recall_score(y_test['TARGET_B'],pred, pos_label= 0))\n",
    "print(\"f1: \",f1_score(y_test['TARGET_B'],pred, pos_label= 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feb08ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8a49d6e",
   "metadata": {},
   "source": [
    "# Applying the model(Random Forest Regretion) to predict Donations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539a79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the application of the SMOTE algorithm improves the model a bit with the prediction moving from 257 to 431; an increase \n",
    "# of 174 True positives\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LogReg = LogisticRegression(random_state=0, solver='lbfgs')\n",
    "LogReg.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "pred = LogReg.predict(X_test_scaled)\n",
    "print(\"accuracy:\",accuracy_score(y_test, pred))\n",
    "print(\"precision: \",precision_score(y_test,pred, pos_label='Yes'))\n",
    "print(\"recall: \",recall_score(y_test,pred, pos_label='Yes'))\n",
    "print(\"f1: \",f1_score(y_test,pred, pos_label='Yes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a30835e",
   "metadata": {},
   "source": [
    "# Applying the model(Random Forest Regretion) to predict Donations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc33839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4709b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = DecisionTreeRegressor(max_depth=5)\n",
    "model = regr.fit(X_test_treated, data_upsampled)\n",
    "\n",
    "print(\"test data R2 score was: \",regr.score(X_test_treated, y_test))\n",
    "print(\"train data R2 score was: \",regr.score(X_train_treated, data_upsampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babdc97e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
